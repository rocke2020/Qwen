{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<|endoftext|>': 151643, '<|im_start|>': 151644, '<|im_end|>': 151645, '<|extra_0|>': 151646, '<|extra_1|>': 151647, '<|extra_2|>': 151648, '<|extra_3|>': 151649, '<|extra_4|>': 151650, '<|extra_5|>': 151651, '<|extra_6|>': 151652, '<|extra_7|>': 151653, '<|extra_8|>': 151654, '<|extra_9|>': 151655, '<|extra_10|>': 151656, '<|extra_11|>': 151657, '<|extra_12|>': 151658, '<|extra_13|>': 151659, '<|extra_14|>': 151660, '<|extra_15|>': 151661, '<|extra_16|>': 151662, '<|extra_17|>': 151663, '<|extra_18|>': 151664, '<|extra_19|>': 151665, '<|extra_20|>': 151666, '<|extra_21|>': 151667, '<|extra_22|>': 151668, '<|extra_23|>': 151669, '<|extra_24|>': 151670, '<|extra_25|>': 151671, '<|extra_26|>': 151672, '<|extra_27|>': 151673, '<|extra_28|>': 151674, '<|extra_29|>': 151675, '<|extra_30|>': 151676, '<|extra_31|>': 151677, '<|extra_32|>': 151678, '<|extra_33|>': 151679, '<|extra_34|>': 151680, '<|extra_35|>': 151681, '<|extra_36|>': 151682, '<|extra_37|>': 151683, '<|extra_38|>': 151684, '<|extra_39|>': 151685, '<|extra_40|>': 151686, '<|extra_41|>': 151687, '<|extra_42|>': 151688, '<|extra_43|>': 151689, '<|extra_44|>': 151690, '<|extra_45|>': 151691, '<|extra_46|>': 151692, '<|extra_47|>': 151693, '<|extra_48|>': 151694, '<|extra_49|>': 151695, '<|extra_50|>': 151696, '<|extra_51|>': 151697, '<|extra_52|>': 151698, '<|extra_53|>': 151699, '<|extra_54|>': 151700, '<|extra_55|>': 151701, '<|extra_56|>': 151702, '<|extra_57|>': 151703, '<|extra_58|>': 151704, '<|extra_59|>': 151705, '<|extra_60|>': 151706, '<|extra_61|>': 151707, '<|extra_62|>': 151708, '<|extra_63|>': 151709, '<|extra_64|>': 151710, '<|extra_65|>': 151711, '<|extra_66|>': 151712, '<|extra_67|>': 151713, '<|extra_68|>': 151714, '<|extra_69|>': 151715, '<|extra_70|>': 151716, '<|extra_71|>': 151717, '<|extra_72|>': 151718, '<|extra_73|>': 151719, '<|extra_74|>': 151720, '<|extra_75|>': 151721, '<|extra_76|>': 151722, '<|extra_77|>': 151723, '<|extra_78|>': 151724, '<|extra_79|>': 151725, '<|extra_80|>': 151726, '<|extra_81|>': 151727, '<|extra_82|>': 151728, '<|extra_83|>': 151729, '<|extra_84|>': 151730, '<|extra_85|>': 151731, '<|extra_86|>': 151732, '<|extra_87|>': 151733, '<|extra_88|>': 151734, '<|extra_89|>': 151735, '<|extra_90|>': 151736, '<|extra_91|>': 151737, '<|extra_92|>': 151738, '<|extra_93|>': 151739, '<|extra_94|>': 151740, '<|extra_95|>': 151741, '<|extra_96|>': 151742, '<|extra_97|>': 151743, '<|extra_98|>': 151744, '<|extra_99|>': 151745, '<|extra_100|>': 151746, '<|extra_101|>': 151747, '<|extra_102|>': 151748, '<|extra_103|>': 151749, '<|extra_104|>': 151750, '<|extra_105|>': 151751, '<|extra_106|>': 151752, '<|extra_107|>': 151753, '<|extra_108|>': 151754, '<|extra_109|>': 151755, '<|extra_110|>': 151756, '<|extra_111|>': 151757, '<|extra_112|>': 151758, '<|extra_113|>': 151759, '<|extra_114|>': 151760, '<|extra_115|>': 151761, '<|extra_116|>': 151762, '<|extra_117|>': 151763, '<|extra_118|>': 151764, '<|extra_119|>': 151765, '<|extra_120|>': 151766, '<|extra_121|>': 151767, '<|extra_122|>': 151768, '<|extra_123|>': 151769, '<|extra_124|>': 151770, '<|extra_125|>': 151771, '<|extra_126|>': 151772, '<|extra_127|>': 151773, '<|extra_128|>': 151774, '<|extra_129|>': 151775, '<|extra_130|>': 151776, '<|extra_131|>': 151777, '<|extra_132|>': 151778, '<|extra_133|>': 151779, '<|extra_134|>': 151780, '<|extra_135|>': 151781, '<|extra_136|>': 151782, '<|extra_137|>': 151783, '<|extra_138|>': 151784, '<|extra_139|>': 151785, '<|extra_140|>': 151786, '<|extra_141|>': 151787, '<|extra_142|>': 151788, '<|extra_143|>': 151789, '<|extra_144|>': 151790, '<|extra_145|>': 151791, '<|extra_146|>': 151792, '<|extra_147|>': 151793, '<|extra_148|>': 151794, '<|extra_149|>': 151795, '<|extra_150|>': 151796, '<|extra_151|>': 151797, '<|extra_152|>': 151798, '<|extra_153|>': 151799, '<|extra_154|>': 151800, '<|extra_155|>': 151801, '<|extra_156|>': 151802, '<|extra_157|>': 151803, '<|extra_158|>': 151804, '<|extra_159|>': 151805, '<|extra_160|>': 151806, '<|extra_161|>': 151807, '<|extra_162|>': 151808, '<|extra_163|>': 151809, '<|extra_164|>': 151810, '<|extra_165|>': 151811, '<|extra_166|>': 151812, '<|extra_167|>': 151813, '<|extra_168|>': 151814, '<|extra_169|>': 151815, '<|extra_170|>': 151816, '<|extra_171|>': 151817, '<|extra_172|>': 151818, '<|extra_173|>': 151819, '<|extra_174|>': 151820, '<|extra_175|>': 151821, '<|extra_176|>': 151822, '<|extra_177|>': 151823, '<|extra_178|>': 151824, '<|extra_179|>': 151825, '<|extra_180|>': 151826, '<|extra_181|>': 151827, '<|extra_182|>': 151828, '<|extra_183|>': 151829, '<|extra_184|>': 151830, '<|extra_185|>': 151831, '<|extra_186|>': 151832, '<|extra_187|>': 151833, '<|extra_188|>': 151834, '<|extra_189|>': 151835, '<|extra_190|>': 151836, '<|extra_191|>': 151837, '<|extra_192|>': 151838, '<|extra_193|>': 151839, '<|extra_194|>': 151840, '<|extra_195|>': 151841, '<|extra_196|>': 151842, '<|extra_197|>': 151843, '<|extra_198|>': 151844, '<|extra_199|>': 151845, '<|extra_200|>': 151846, '<|extra_201|>': 151847, '<|extra_202|>': 151848, '<|extra_203|>': 151849, '<|extra_204|>': 151850}\n",
      "151644 151645 151643\n",
      "151643 IQ== 0\n",
      " 4r2X 151642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from icecream import ic\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "\n",
    "\n",
    "def load_tokenizer():\n",
    "    \"\"\"  \n",
    "    Currently, used special tokens are `<|endoftext|>` in Qwen-7B, and `<|endoftext|>`, \n",
    "    `<|im_start|>`, and `<|im_end|>` in Qwen-7B-Chat,    \n",
    "    \"\"\"\n",
    "    cache_dir = None\n",
    "    model_max_length = 512\n",
    "    model_name_or_path = '/mnt/nas1/models/qwen/Qwen-7B-Chat-Int8'\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        cache_dir=cache_dir,\n",
    "        model_max_length=model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    tokenizer.pad_token_id = tokenizer.eod_id\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "tokenizer = load_tokenizer()\n",
    "end_token = \"<|endoftext|>\"\n",
    "im_start = tokenizer.im_start_id\n",
    "im_end = tokenizer.im_end_id\n",
    "eod_id = tokenizer.eod_id\n",
    "roles = {\"user\": \"<|im_start|>user\", \"assistant\": \"<|im_start|>assistant\"}\n",
    "existent_token_file = '/mnt/nas1/models/qwen/Qwen-7B-Chat-Int8/qwen.tiktoken'\n",
    "print(tokenizer.special_tokens)\n",
    "print(im_start, im_end, eod_id)\n",
    "with open(existent_token_file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "print(len(lines), lines[0], lines[-1])\n",
    "# tokenizer convert to bytes level, not char level\n",
    "# word 一只 is already a token b'\\xe4\\xb8\\x80\\xe5\\x8f\\xaa', skipping\n",
    "# for line in lines:\n",
    "#     if '\\xe4\\xb8\\x80\\xe5\\x8f\\xaa' in line:\n",
    "#         print(line)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一只'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b'\\xe4\\xb8\\x80\\xe5\\x8f\\xaa'.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14990, 1879]\n",
      "{'input_ids': [1350, 9639, 91, 8691, 723, 427, 91, 82598], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [1350, 9639, 91, 15460, 62, 15, 91, 82598, 151643], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"hello world\"))\n",
    "print(tokenizer('print(\"<|endoftext|>\")', allowed_special=set()))\n",
    "print(tokenizer('print(\"<|extra_0|>\")<|endoftext|>', allowed_special={'<|endoftext|>'}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ll",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
