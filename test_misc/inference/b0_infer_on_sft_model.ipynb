{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qcdong/anaconda3/envs/ll/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ic| 99688295.py:44 in <module>- loads tokenizer done\n",
      "ic| 99688295.py:48 in <module>\n",
      "    generation_config: GenerationConfig {\n",
      "                         \"chat_format\": \"chatml\",\n",
      "                         \"do_sample\": true,\n",
      "                         \"eos_token_id\": 151643,\n",
      "                         \"max_new_tokens\": 512,\n",
      "                         \"max_window_size\": 6144,\n",
      "                         \"pad_token_id\": 151643,\n",
      "                         \"repetition_penalty\": 1.1,\n",
      "                         \"top_k\": 0,\n",
      "                         \"top_p\": 0.8,\n",
      "                         \"trust_remote_code\": true\n",
      "                       }\n",
      "ic| 99688295.py:52 in <module>\n",
      "    config: LoraConfig(peft_type='LORA', auto_mapping=None, base_model_name_or_path='/mnt/nas1/models/qwen/Qwen-7B-Chat-Int8', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=64, target_modules=['c_attn', 'c_proj', 'w1', 'w2'], lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n",
      "23-11-22 20:41 modeling_qwen.py 999: Warning: please make sure that you are using the latest codes and checkpoints, especially if you used Qwen-7B before 09.25.2023.请使用最新模型和代码，尤其如果你在9月25日前已经开始使用Qwen-7B，千万注意不要使用错误代码和模型。\n",
      "23-11-22 20:41 modeling_qwen.py 1035: Try importing flash-attention for faster inference...\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:38<00:00,  7.63s/it]\n",
      "ic| 99688295.py:73 in <module>- directly loads peft model done\n",
      "ic| 99688295.py:74 in <module>- model.device: cpu\n",
      "ic| 99688295.py:77 in <module>- device: cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国在十四五规划中提出了“五大基础研究和关键技术突破”重点任务，其中涉及的重点领域产业关键与共性技术攻关工程包括：\n",
      "\n",
      "1. 基础研究：人工智能、量子信息科学、生物技术、先进制造等前沿领域。\n",
      "\n",
      "2. 关键核心技术：新一代信息技术、高端装备、新材料、新能源汽车、节能环保等战略性新兴产业的关键核心技术。\n",
      "\n",
      "3. 共性技术：数字经济、生命健康、新一代信息基础设施等领域的重要共性技术。\n",
      "\n",
      "4. 产业融合：将新兴技术与传统行业深度融合，促进传统产业转型升级，如智能制造、数字农业、智能医疗等。\n",
      "\n",
      "5. 深度应用：在各领域深度应用新技术，推动经济高质量发展，如智慧城市、智慧交通、智能家居等。\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from icecream import ic\n",
    "from pandas import DataFrame\n",
    "from peft import AutoPeftModelForCausalLM, PeftConfig, PeftModel\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "sys.path.append(os.path.abspath(\"/home/qcdong/codes/Qwen\"))\n",
    "from qwen_generation_utils import decode_tokens, make_context\n",
    "\n",
    "ic.configureOutput(includeContext=True, argToStringFunction=str)\n",
    "ic.lineWrapWidth = 120\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%y-%m-%d %H:%M\",\n",
    "    format=\"%(asctime)s %(filename)s %(lineno)d: %(message)s\",\n",
    ")\n",
    "\n",
    "\n",
    "model_max_length = 768\n",
    "model_name_or_path = \"/mnt/nas1/models/qwen/Qwen-7B-Chat-Int8\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    model_max_length=768,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eod_id\n",
    "ic(\"loads tokenizer done\")\n",
    "generation_config = GenerationConfig.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=True\n",
    ")\n",
    "ic(generation_config)\n",
    "\n",
    "peft_model_path = \"/mnt/nas1/models/qwen/Qwen-7B-Chat-int8-moss-small\"\n",
    "config = PeftConfig.from_pretrained(peft_model_path)\n",
    "ic(config)\n",
    "\n",
    "load_basic_model = 0\n",
    "load_in_normal_order = 0\n",
    "if load_basic_model:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path, trust_remote_code=True\n",
    "    )\n",
    "    ic(\"loads base model done\")\n",
    "else:\n",
    "    if load_in_normal_order:\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            model_name_or_path, trust_remote_code=True\n",
    "        )\n",
    "        ic(\"loads base model done\")\n",
    "        model = PeftModel.from_pretrained(model, peft_model_path)\n",
    "        ic(\"loads peft model done\")\n",
    "    else:\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            peft_model_path, trust_remote_code=True\n",
    "        )\n",
    "        ic(\"directly loads peft model done\")\n",
    "ic(model.device)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ic(device)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "model.generation_config = generation_config\n",
    "\n",
    "query = \"重点领域产业关键与共性技术攻关工程有哪些?\"\n",
    "response, history = model.chat(tokenizer, query, history=None)\n",
    "# base model response: 《“十四五”国家科技创新规划》提出，实施重点领域产业关键与共性技术攻关工程。聚焦芯片与集成电路、前沿新材料、生命健康、人工智能、新能源、节能环保等重点产业领域，强化前瞻性基础研究和引领性原创成果重大突破，实现重大标志性原创成果重大突破。\n",
    "print(response)\n",
    "\n",
    "# all_raw_text = [\"重点领域产业关键与共性技术攻关工程有哪些\"]\n",
    "# batch_raw_text = []\n",
    "# for q in all_raw_text:\n",
    "#     raw_text, _ = make_context(\n",
    "#         tokenizer,\n",
    "#         q,\n",
    "#         system=\"You are a helpful assistant.\",\n",
    "#         max_window_size=model.generation_config.max_window_size,\n",
    "#         chat_format=model.generation_config.chat_format,\n",
    "#     )\n",
    "#     batch_raw_text.append(raw_text)\n",
    "\n",
    "# batch_input_ids = tokenizer(batch_raw_text, padding=\"longest\")\n",
    "# batch_input_ids = torch.LongTensor(batch_input_ids[\"input_ids\"]).to(model.device)\n",
    "# ic(batch_input_ids.size())\n",
    "# batch_out_ids = model.generate(\n",
    "#     batch_input_ids,\n",
    "#     return_dict_in_generate=False,\n",
    "#     generation_config=model.generation_config,\n",
    "# )\n",
    "# padding_lens = [\n",
    "#     batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item()\n",
    "#     for i in range(batch_input_ids.size(0))\n",
    "# ]\n",
    "# ic(padding_lens)\n",
    "# batch_response = [\n",
    "#     decode_tokens(\n",
    "#         batch_out_ids[i][padding_lens[i] :],\n",
    "#         tokenizer,\n",
    "#         raw_text_len=len(batch_raw_text[i]),\n",
    "#         context_length=(batch_input_ids[i].size(0) - padding_lens[i]),\n",
    "#         chat_format=\"chatml\",\n",
    "#         verbose=False,\n",
    "#         errors=\"replace\",\n",
    "#     )\n",
    "#     for i in range(len(all_raw_text))\n",
    "# ]\n",
    "# print(batch_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 2084782153.py:30 in <module>- len(outputs[0]): 542\n",
      "ic| 2084782153.py:31 in <module>\n",
      "    tokenizer.decode(outputs[0]): <|im_start|>system\n",
      "                                  You are a helpful assistant.<|im_end|>\n",
      "                                  <|im_start|>user\n",
      "                                  重点领域产业关键与共性技术攻关工程有哪些?<|im_end|>\n",
      "                                  <|im_start|>assistant\n",
      "                                  中国的重点领域产业关键与共性技术攻关工程包括以下几个方面：\n",
      "                                  \n",
      "                                  1. 节能环保：主要针对节能减排、资源循环利用、环境污染治理等方面的技术研发。\n",
      "                                  \n",
      "                                  2. 智能制造：主要涉及智能制造装备、工业互联网、智能工厂等领域，旨在提高制造业的智能化水平和生产效率。\n",
      "                                  \n",
      "                                  3. 人工智能：主要研究机器学习、自然语言处理、计算机视觉等领域的关键技术，以推动人工智能应用的发展。\n",
      "                                  \n",
      "                                  4. 生物医药：主要研究药物研发、基因编辑、生物制药等领域，以解决医疗健康领域的问题。\n",
      "                                  \n",
      "                                  5. 新材料：主要研究新型材料的制备、性能优化等关键技术，以推动新材料在各个领域的应用。\n",
      "                                  \n",
      "                                  6. 空间科技：主要研究空间探测、卫星通信、航天器设计等领域，以推动空间技术的发展。\n",
      "                                  \n",
      "                                  7. 物联网：主要研究物联网设备、传感器、网络传输等关键技术，以实现各类物体之间的互联互通。\n",
      "                                  \n",
      "                                  8. 金融科技：主要研究区块链、云计算、大数据等金融科技核心技术，以推动金融行业的数字化转型。<|im_end|>\n",
      "                                  <|im_start|>'t<|im_end|>\n",
      "                                  <|im_start|><|im_end|>\n",
      "                                  <|im_start|>0<|im_end|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|><|im_end|>\n",
      "                                  <|im_start|><|im_end|>\n",
      "                                  <|im_start|><|im_end|>\n",
      "                                  <|im_start|><|im_end|>\n",
      "                                  <|im_start|><|im_end|>\n",
      "                                  <|im_start|><|im_start|>\n",
      "                                  <|im_start|><|im_end|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|><|im_end|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n",
      "                                  <|im_start|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国的重点领域产业关键与共性技术攻关工程包括以下几个方面：\n",
      "\n",
      "1. 节能环保：主要针对节能减排、资源循环利用、环境污染治理等方面的技术研发。\n",
      "\n",
      "2. 智能制造：主要涉及智能制造装备、工业互联网、智能工厂等领域，旨在提高制造业的智能化水平和生产效率。\n",
      "\n",
      "3. 人工智能：主要研究机器学习、自然语言处理、计算机视觉等领域的关键技术，以推动人工智能应用的发展。\n",
      "\n",
      "4. 生物医药：主要研究药物研发、基因编辑、生物制药等领域，以解决医疗健康领域的问题。\n",
      "\n",
      "5. 新材料：主要研究新型材料的制备、性能优化等关键技术，以推动新材料在各个领域的应用。\n",
      "\n",
      "6. 空间科技：主要研究空间探测、卫星通信、航天器设计等领域，以推动空间技术的发展。\n",
      "\n",
      "7. 物联网：主要研究物联网设备、传感器、网络传输等关键技术，以实现各类物体之间的互联互通。\n",
      "\n",
      "8. 金融科技：主要研究区块链、云计算、大数据等金融科技核心技术，以推动金融行业的数字化转型。\n"
     ]
    }
   ],
   "source": [
    "from qwen_generation_utils import decode_tokens, make_context, get_stop_words_ids\n",
    "from icecream import ic\n",
    "ic.configureOutput(includeContext=True, argToStringFunction=str)\n",
    "ic.lineWrapWidth = 120\n",
    "\n",
    "stop_words_ids = []\n",
    "max_window_size = generation_config.max_window_size\n",
    "system = 'You are a helpful assistant.'\n",
    "history = []\n",
    "raw_text, context_tokens = make_context(\n",
    "    tokenizer,\n",
    "    query,\n",
    "    history=history,\n",
    "    system=system,\n",
    "    max_window_size=max_window_size,\n",
    "    chat_format=generation_config.chat_format,\n",
    ")\n",
    "\n",
    "stop_words_ids.extend(get_stop_words_ids(\n",
    "    generation_config.chat_format, tokenizer\n",
    "))\n",
    "\n",
    "input_ids = torch.tensor([context_tokens]).to(model.device)\n",
    "print(type(model))\n",
    "outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            return_dict_in_generate=False,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "ic(len(outputs[0]))\n",
    "ic(tokenizer.decode(outputs[0]))\n",
    "response = decode_tokens(\n",
    "    outputs[0],\n",
    "    tokenizer,\n",
    "    raw_text_len=len(raw_text),\n",
    "    context_length=len(context_tokens),\n",
    "    chat_format=generation_config.chat_format,\n",
    "    verbose=False,\n",
    "    errors='replace'\n",
    ")\n",
    "\n",
    "# as history is a copy of the user inputs,\n",
    "# we can always return the new turn to the user.\n",
    "# separating input history and output history also enables the user\n",
    "# to implement more complex history management\n",
    "history.append((query, response))\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ll",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
